{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autobus - Live demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############import libraries, create helper functions, *.pth files, etc.#############\n",
    "import time\n",
    "print(time.ctime(),'go')\n",
    "\n",
    "######Make Robot#######\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "from jetbot import Robot\n",
    "robot = Robot()\n",
    "\n",
    "######UI functions######\n",
    "import traitlets\n",
    "import ipywidgets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "######Monitor Network Connection#######\n",
    "from jetbot import Heartbeat\n",
    "def handle_heartbeat_status(change):\n",
    "    if change['new'] == Heartbeat.Status.dead:\n",
    "        print('check network connection')\n",
    "heartbeat = Heartbeat(period=5)\n",
    "heartbeat.observe(handle_heartbeat_status, names='status') # attach the callback function to heartbeat status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rail Tracking Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Rail Tracking Setup#######\n",
    "import torchvision\n",
    "import torch\n",
    "steer_model_file = 'best_steering_model_xy--PoolTable400.pth'\n",
    "steer_model = torchvision.models.resnet18(pretrained=False)\n",
    "steer_model.fc = torch.nn.Linear(512, 2)\n",
    "steer_model.load_state_dict(torch.load(steer_model_file))\n",
    "steer_device = torch.device('cuda')\n",
    "steer_model = steer_model.to(steer_device)\n",
    "steer_model = steer_model.eval().half()\n",
    "print(time.ctime(),steer_model_file,'ok')\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "steer_mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "steer_std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "def steer_preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(steer_device).half()\n",
    "    image.sub_(steer_mean[:, None, None]).div_(steer_std[:, None, None])\n",
    "    return image[None, ...]\n",
    "\n",
    "#add the steering xy prediction to the image\n",
    "def display_xy(camera_image):\n",
    "    image = np.copy(camera_image)\n",
    "    x = x_slider.value\n",
    "    y = y_slider.value\n",
    "    x = int(x * 224 / 2 + 112)\n",
    "    y = int(y * 224 / 2 + 112)\n",
    "    image = cv2.circle(image, (x, y), 8, (0, 255, 0), 3)\n",
    "    image = cv2.circle(image, (112, 224), 8, (0, 0,255), 3)\n",
    "    image = cv2.line(image, (x,y), (112,224), (255,0,0), 3)\n",
    "    jpeg_image = bgr8_to_jpeg(image)\n",
    "    return jpeg_image\n",
    "\n",
    "print(time.ctime(),'ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Avoidance Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Collision Avoidance Setup#######\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "oba_model = torchvision.models.alexnet(pretrained=False)\n",
    "oba_model.classifier[6] = torch.nn.Linear(oba_model.classifier[6].in_features, 2)\n",
    "#oba_model.load_state_dict(torch.load('best_model.pth'))\n",
    "oba_model.load_state_dict(torch.load('best_obstacle_model--NVIDIA.pth'))\n",
    "oba_device = torch.device('cuda')\n",
    "oba_model = oba_model.to(oba_device)\n",
    "print(time.ctime(),'oba_model','ok')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "oba_mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "oba_stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "oba_normalize = torchvision.transforms.Normalize(oba_mean, oba_stdev)\n",
    "def oba_preprocess(camera_value):\n",
    "    global oba_device, oba_normalize\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = oba_normalize(x)\n",
    "    x = x.to(oba_device)\n",
    "    x = x[None, ...]\n",
    "    return x\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "print(time.ctime(),'ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that will get called whenever the camera's value changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a function that will get called whenever the camera's value changes. \n",
    "#This function will do the following steps\n",
    "#- Pre-process the camera image\n",
    "#- Execute the neural network\n",
    "#- Stop and prevent motion on obstacle detection\n",
    "#- Stop when jetbot reaches a station, resumes after a set time\n",
    "#- If OK to move, control steering, keep jetbot on the track\n",
    "\n",
    "######Global vars for execute#######\n",
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "mt = 0\n",
    "frame_cnt = 0\n",
    "interval = 0.1\n",
    "stutter = 1\n",
    "debug = False\n",
    "pp = time.time()\n",
    "\n",
    "######Jetbot doing something with new image#######\n",
    "def execute(change):\n",
    "    global angle, angle_last, mt, frame_cnt, pp, debug, interval, accel, stutter, robot\n",
    "    \n",
    "    #count images read\n",
    "    frame_cnt += 1\n",
    "    \n",
    "    #min time interval between each run\n",
    "    if time.time() < pp + interval:\n",
    "        return\n",
    "    pp = time.time()\n",
    "    \n",
    "    #stutters robot, slows movement for turns\n",
    "    if stutter:\n",
    "        robot.left_motor.value = 0.1 *mt\n",
    "        robot.right_motor.value = 0.1 *mt\n",
    "        #time.sleep(0.1) #stop to get clear image\n",
    "        \n",
    "    #the new image\n",
    "    image = change['new']\n",
    "\n",
    "    ##AAAAAAAAAAA jetbot reads image and makes a obstacle stop decision ######\n",
    "    x = change['new'] \n",
    "    x = oba_preprocess(x)\n",
    "    y = oba_model(x)\n",
    "    \n",
    "    # we apply the `softmax` function to normalize the output vector so it sums to 1 (which makes it a probability distribution)\n",
    "    y = F.softmax(y, dim=1)\n",
    "    \n",
    "    prob_blocked = float(y.flatten()[0])\n",
    "    if prob_blocked > 0.6:\n",
    "        robot.left_motor.value = 0\n",
    "        robot.right_motor.value = 0\n",
    "        status_a.value = str(frame_cnt) + \" obstacle prob: \" + \"{:.2f}\".format(prob_blocked)\n",
    "        return\n",
    "    ##AAAAAAAAAAA jetbot finishes obstacle stop decision ######################\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ##BBBBBBBBBB jetbot reads image and makes a station stop decision #########\n",
    "    \n",
    "    # extracting out HSV color space values:\n",
    "    # H – Hue (Dominant Wavelength)\n",
    "    # S – Saturation (Purity/Shades of Color)\n",
    "    # V – Value (Intensity)\n",
    "    hsv = cv2.cvtColor(change['new'], cv2.COLOR_BGR2HSV)\n",
    "    # adding median blur to image\n",
    "    hsv = cv2.medianBlur(hsv,5)\n",
    "    print(hsv)\n",
    "    # lower range of red\n",
    "    bttm_lower_red = np.array([0,120,70])\n",
    "    top_lower_red = np.array([10,255,255])\n",
    "    lower_red_mask = cv2.inRange(hsv, bttm_lower_red, top_lower_red)\n",
    "    # upper range of red\n",
    "    bttm_upper_red = np.array([170,120,70])\n",
    "    top_upper_red = np.array([180,255,255])\n",
    "    upper_red_mask = cv2.inRange(hsv, bttm_upper_red, top_upper_red)\n",
    "    # generating the final mask to detect red color\n",
    "    red_range_mask = lower_red_mask + upper_red_mask\n",
    "    # removing noise through opening (erosion followed by dilation)\n",
    "    red_range_mask = cv2.morphologyEx(red_range_mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "    # increasing white region or size of foreground object increases through dilation\n",
    "    red_range_mask = cv2.morphologyEx(red_range_mask, cv2.MORPH_DILATE, np.ones((3,3),np.uint8))\n",
    "    # detecting if image frame contains red\n",
    "    red_results = cv2.bitwise_and(image_frame, image_frame, mask = red_range_mask)\n",
    "    # lower range of yellow\n",
    "    bttm_lower_yellow = np.array([0,120,70]) # TODO: change values\n",
    "    top_lower_yellow = np.array([10,255,255]) # TODO: change values\n",
    "    lower_yellow_mask = cv2.inRange(hsv, bttm_lower_yellow, top_lower_yellow)\n",
    "    # upper range of yellow\n",
    "    bttm_upper_yellow = np.array([170,120,70]) # TODO: change values\n",
    "    top_upper_yellow = np.array([180,255,255]) # TODO: change values\n",
    "    upper_yellow_mask = cv2.inRange(hsv, bttm_upper_yellow, top_upper_yellow)\n",
    "    # generating the final mask to detect yellow color\n",
    "    yellow_range_mask = lower_yellow_mask + upper_yellow_mask\n",
    "    # removing noise through opening (erosion followed by dilation)\n",
    "    yellow_range_mask = cv2.morphologyEx(yellow_range_mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "    # increasing white region or size of foreground object increases through dilation\n",
    "    yellow_range_mask = cv2.morphologyEx(yellow_range_mask, cv2.MORPH_DILATE, np.ones((3,3),np.uint8))\n",
    "    # detecting if image frame contains yellow\n",
    "    yellow_results = cv2.bitwise_and(image_frame, image_frame, mask = yellow_range_mask)\n",
    "    # lower range of green\n",
    "    bttm_lower_green = np.array([0,120,70]) # TODO: change values\n",
    "    top_lower_green = np.array([10,255,255]) # TODO: change values\n",
    "    lower_green_mask = cv2.inRange(hsv, bttm_lower_green, top_lower_green)\n",
    "    # upper range of green\n",
    "    bttm_upper_green = np.array([170,120,70]) # TODO: change values\n",
    "    top_upper_green = np.array([180,255,255]) # TODO: change values\n",
    "    upper_green_mask = cv2.inRange(hsv, bttm_upper_green, top_upper_green)\n",
    "    # generating the final mask to detect green color\n",
    "    green_range_mask = lower_green_mask + upper_green_mask\n",
    "    # removing noise through opening (erosion followed by dilation)\n",
    "    green_range_mask = cv2.morphologyEx(green_range_mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "    # increasing white region or size of foreground object increases through dilation\n",
    "    green_range_mask = cv2.morphologyEx(green_range_mask, cv2.MORPH_DILATE, np.ones((3,3),np.uint8))\n",
    "    # detecting if image frame contains green\n",
    "    green_results = cv2.bitwise_and(image_frame, image_frame, mask = green_range_mask)\n",
    "    # lower range of purple\n",
    "    bttm_lower_purple = np.array([0,120,70]) # TODO: change values\n",
    "    top_lower_purple = np.array([10,255,255]) # TODO: change values\n",
    "    lower_purple_mask = cv2.inRange(hsv, bttm_lower_purple, top_lower_purple)\n",
    "    # upper range of purple\n",
    "    bttm_upper_purple = np.array([170,120,70]) # TODO: change values\n",
    "    top_upper_purple = np.array([180,255,255]) # TODO: change values\n",
    "    upper_purple_mask = cv2.inRange(hsv, bttm_purple_green, top_purple_green)\n",
    "    # generating the final mask to detect purple color\n",
    "    purple_range_mask = lower_purple_mask + upper_purple_mask\n",
    "    # removing noise through opening (erosion followed by dilation)\n",
    "    purple_range_mask = cv2.morphologyEx(purple_range_mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "    # increasing white region or size of foreground object increases through dilation\n",
    "    purple_range_mask = cv2.morphologyEx(purple_range_mask, cv2.MORPH_DILATE, np.ones((3,3),np.uint8))\n",
    "    # detecting if image frame contains purple\n",
    "    purple_results = cv2.bitwise_and(image_frame, image_frame, mask = purple_range_mask)\n",
    "    if red_results is not None:\n",
    "        # save detected color value as red\n",
    "        color_detected = ‘red’\n",
    "    elif yellow_results is not None:\n",
    "        # save detected color value as yellow\n",
    "        color_detected = ‘yellow’\n",
    "    elif green_results is not None:\n",
    "        # save detected color value as green\n",
    "        color_detected = ‘green’\n",
    "    elif purple_results is not None:\n",
    "        # save detected color value as purple\n",
    "        color_detected = ‘purple’\n",
    "    else:\n",
    "        # save detected color value as other\n",
    "        color_detected = ‘other’\n",
    "    # output deteced color value\n",
    "    status_b.value = str(frame_cnt) + \" color detected: \" + color_detected\n",
    "    \n",
    "    ##BBBBBBBBBB jetbot finishes station stop decision #########\n",
    "\n",
    "    \n",
    "    ##CCCCCCCCCCC jetbot reads image and makes a steering decision ############\n",
    "    xy = steer_model(steer_preprocess(image)).detach().float().cpu().numpy().flatten()\n",
    "    x = xy[0]\n",
    "    y = (0.5 - xy[1]) / 2.0\n",
    "    \n",
    "    x_slider.value = x\n",
    "    y_slider.value = y\n",
    "    \n",
    "    speed_slider.value = speed_gain_slider.value\n",
    "\n",
    "    angle = np.arctan2(x, y)\n",
    "    pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\n",
    "    angle_last = angle\n",
    "\n",
    "    steering_slider.value = pid + steering_bias_slider.value\n",
    "   \n",
    "    #use default NVIDA algo\n",
    "    if not steer_debug:\n",
    "        robot.left_motor.value = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0) *mt\n",
    "        robot.right_motor.value = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0) *mt\n",
    "        interval = fps #set same as frame rate\n",
    "        \n",
    "        \n",
    "    #########use AUTOBUS debugging algo##########\n",
    "    else: \n",
    "        left_motor = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0) \n",
    "        right_motor = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0) \n",
    "        left_slider.value = left_motor\n",
    "        right_slider.value = right_motor\n",
    "\n",
    "        #run at preset speeds\n",
    "        thres = turn_thres.value #steering value must exceed threshold to initate turning\n",
    "        if steering_slider.value > thres:\n",
    "            status_a.value = str(frame_cnt) + \" turn right\"\n",
    "            robot.left_motor.value = max(robot.left_motor.value + 0.1, 0.4) *mt\n",
    "            robot.right_motor.value = max(robot.right_motor.value + 0.1, 0.3) *mt\n",
    "            interval = min(0.05, interval - 0.01) #decrease interval            \n",
    "            stutter = 1\n",
    "            \n",
    "        elif steering_slider.value < -thres:\n",
    "            status_a.value = str(frame_cnt) + \" turn left\"\n",
    "            robot.left_motor.value = max(robot.left_motor.value + 0.1, 0.3) *mt\n",
    "            robot.right_motor.value = max(robot.right_motor.value + 0.1, 0.45) *mt\n",
    "            interval = min(0.05, interval - 0.01) #decrease interval            \n",
    "            stutter = 1\n",
    "\n",
    "        else: \n",
    "            status_a.value = str(frame_cnt) + \" go straight\"\n",
    "            if robot.left_motor.value != robot.right_motor.value:\n",
    "                robot.left_motor.value = robot.right_motor.value = min(robot.left_motor.value,robot.right_motor.value)\n",
    "            #initial throttle then lower to coast speed    \n",
    "            robot.left_motor.value = (robot.left_motor.value - 0.01) *mt\n",
    "            robot.right_motor.value = (robot.right_motor.value - 0.01) *mt\n",
    "            if min(robot.left_motor.value,robot.right_motor.value) < 0.2:\n",
    "                robot.left_motor.value = robot.right_motor.value = 0.3 *mt  \n",
    "            stutter = 0\n",
    "            interval = max(interval+0.01, 0.1) #increase interval\n",
    "\n",
    "        #set AUTOBUS adjusted value shown in UI\n",
    "        left_adjusted.value = robot.left_motor.value\n",
    "        right_adjusted.value = robot.right_motor.value\n",
    "        ##CCCCCCCCCCCCCCCCCCC jetbot finished steering decision ##################\n",
    "    \n",
    "    return None\n",
    "    \n",
    "print(time.ctime(),'ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup UI - camera view, buttons, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########setup interface - camera view, buttons, etc.#############\n",
    "\n",
    "#user controlled sliders\n",
    "#1. Speed Control (speed_gain_slider): To start your JetBot increase ``speed_gain_slider`` \n",
    "#2. Steering Gain Control (steering_gain_sloder): If you see JetBot is woblling, you need to reduce ``steering_gain_slider`` till it is smooth\n",
    "#3. Steering Bias control (steering_bias_slider): If you see JetBot is biased towards extreme right or extreme left side of the track, you should control this slider till JetBot start following line or track in the center.  This accounts for motor biases as well as camera offsets\n",
    "#jetbot turns when steering value exceeds threshold (for AUTOBUS steering algo only, not in NVIDIA steering algo)\n",
    "speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=0.4, step=0.01, value=0.33, description='speed gain')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=0.4, step=0.01, value=0.33, description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.4, step=0.01, value=0.33, description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, step=0.01, value=0, description='steering bias')\n",
    "turn_thres = ipywidgets.FloatSlider(min=0, max=1.0, step=0.01, value=0.25, description='threshold') #turn threshold\n",
    "\n",
    "#buttons\n",
    "button_layout = ipywidgets.Layout(width='80px', height='35px')\n",
    "run_button = ipywidgets.Button(description='START', button_style='success', layout=button_layout)\n",
    "run_button.on_click(lambda x: start())\n",
    "\n",
    "stop_button = ipywidgets.Button(description='STOP', button_style='danger', layout=button_layout)\n",
    "stop_button.on_click(lambda x: stop())\n",
    "\n",
    "camera_button = ipywidgets.Button(description='CAMERA',layout=button_layout)\n",
    "camera_button.on_click(lambda x: toggle_camera())\n",
    "\n",
    "motor_button = ipywidgets.Button(description='MOTOR',layout=button_layout)\n",
    "motor_button.on_click(lambda x: toggle_motor())\n",
    "\n",
    "stutter_button = ipywidgets.Button(description='STUTTER',layout=button_layout)\n",
    "stutter_button.on_click(lambda x: toggle_stutter())\n",
    "\n",
    "#read only sliders\n",
    "x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, value=0, description='x')\n",
    "y_slider = ipywidgets.FloatSlider(min=0, max=1.0, value=0, orientation='vertical', description='y')\n",
    "steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
    "speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\n",
    "\n",
    "left_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='L nvidia')\n",
    "right_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='R nvidia')\n",
    "\n",
    "left_adjusted = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='L adjusted')\n",
    "right_adjusted = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='R adjusted')\n",
    "\n",
    "#The x and y sliders will display the predicted x, y values.\n",
    "#The steering slider will display our estimated steering value.  Please remember, this value isn't the actual \n",
    "#angle of the target, but simply a value that is nearly proportional.  When the actual angle is ``0``, this \n",
    "#will be zero, and it will increase / decrease with the actual angle.  \n",
    "\n",
    "#arrange widgets in UI\n",
    "status_a = ipywidgets.Textarea(value = \"status a\")\n",
    "status_b = ipywidgets.Textarea(value = \"status b\")\n",
    "sliders = widgets.VBox([speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider,turn_thres])\n",
    "buttons = widgets.VBox([run_button,stop_button,camera_button,motor_button,stutter_button])\n",
    "y_sets = ipywidgets.HBox([y_slider,left_slider,right_slider,left_adjusted,right_adjusted])\n",
    "x_sets = widgets.VBox([x_slider, steering_slider, status_a, status_b])\n",
    "\n",
    "cam_toggle = 1 #enables/disables camera\n",
    "def toggle_camera():\n",
    "    global cam_toggle\n",
    "    if cam_toggle:\n",
    "        camera_link.unlink()\n",
    "        cam_toggle = 0\n",
    "    else:\n",
    "        camera_link.link()\n",
    "        cam_toggle = 1\n",
    "\n",
    "mt = 0 #start/stop motor\n",
    "def toggle_motor():\n",
    "    global mt\n",
    "    if mt: mt = 0\n",
    "    else: mt = 1\n",
    "        \n",
    "stutter = 1 #enable/disable stutter-steps\n",
    "def toggle_stutter():\n",
    "    global stutter\n",
    "    if stutter: stutter = 0\n",
    "    else: stutter = 1\n",
    "\n",
    "#start processing images\n",
    "def start():\n",
    "    execute({'new': camera.value})\n",
    "    camera.observe(execute, names='value')\n",
    "\n",
    "#stop jetbot, stop camera\n",
    "def stop():\n",
    "    time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "    camera.unobserve_all()\n",
    "    robot.stop()\n",
    "    camera.stop()\n",
    "\n",
    "print(time.ctime(),'ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: camera.stop()\n",
    "except: print('check camera')\n",
    "fps = 10\n",
    "camera = Camera.instance(width=224, height=224, capture_width=224, capture_height=224, fps=fps)\n",
    "image_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=display_xy)\n",
    "camera.start()\n",
    "\n",
    "frame_cnt = 0\n",
    "mt = 0 #starts jetbot without moving\n",
    "stutter = 0\n",
    "\n",
    "steer_debug = False #False = default NVIDIA algo True = custom\n",
    "turn_thres.value = 0.3 #only used if steer_debug=True\n",
    "robot.left_motor.alpha = 1 #scales motor speed by this, 1 is no change\n",
    "robot.right_motor.alpha = 1 \n",
    "\n",
    "execute({'new': camera.value}) #test single image run\n",
    "start() #start jetbot without moving (mt=0, MOTOR button to toggle movement)\n",
    "print(time.ctime(),'ok')\n",
    "\n",
    "display(widgets.HBox([image_widget,sliders,buttons]))\n",
    "display(ipywidgets.HBox([y_sets,x_sets]))\n",
    "\n",
    "#IMPORTANT: The camera must be re-initialized after it stops\n",
    "#Rerun this block to reinitialize camera\n",
    "#First run takes a few min to start up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///////////////////////////////////////////////End of Notebook //////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute({'new': camera.value})\n",
    "#camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
